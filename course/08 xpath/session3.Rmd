---
title: "Web Data Collection with R"
author: "Peter Mei√üner / Dominic Nyhuis / 2015-02-13/14 / ECPR WSM"
output:
  beamer_presentation:
    fonttheme: structurebold
    toc: yes
  slidy_presentation: default
subtitle: '![](../fig/session3.pdf)'
keep_tex: yes
---

# Scenario 2 - Overview

## Scenario 2
&nbsp;

### ... in which we ...
- extract **links** using **XPath**
- folow them to **extract links** again 
- and build a **network** of notable political scientists

## Scenario 2
### ... and learn about ... 
- **Xpath**
- **Selector Gadget**

## Scenario 2
### ... while using packages ... 
- **rvest** (information extraction from HTML)
- **stringr** (string manipulation)
- **d3Network** (network data visualizetion)


# Scenario 2 - Live coding

## Scenario 2
### a first glance at the page
```{r, message=FALSE}
require(rvest)
require(stringr)
```

```{r, eval=1:2}
url <- 
"http://en.wikipedia.org/wiki/List_of_political_scientists"

browseURL(url)
```

## Scenario 2
### a first try at extracting links 
- in scenario 1 we had a simple approach for getting the links 
```{r}
html   <- html(url)
ankers <- html_nodes(html, xpath="//a")
length(ankers)
```
- thereafter using RegEx to get rid of those links that did not lead to PDF files
- **we could also use XPath for filtering**



# How XPath works ...

## How XPath works ... 
- XPath is a language to query XML (HTML) documents
- builds on 
    - **hierarchy** (select parent, child, sibling, ... node)
    - **node names** (select node by name)
    - **node values** (select node by value)
    - **attribute name and value** (select node on attribute value)
    - **further functions** (select depending on more complex derivates of the above)
        - e.g. name, string_length, contains, count, position, ... 
    - **operators**
        - e.g. `|`, `+`, `-`, `=`, `!=`, `<=`, `or`, `and`, ...
- allows to extract
    - node values 
    - attribute values
- ... from single nodes and node sets


## How XPath works ... 
### explicit path
```{r}
x="/html/body/div[3]/div[3]/div[4]/ul[1]/li[1]/a"
html_nodes(html, xpath=x)
```

## How XPath works ... 
### path anywhere in hierarchy
```{r}
html_nodes(html, xpath="//ul/li[1]/a")
```

## How XPath works ... 
### path anywhere in hierarchy with clause and function
```{r}
html_nodes(html, 
  xpath="//ul/li/a[text()='Alan Abramowitz']")
```


## How XPath works ... 
### XPath can very fast become mind-buggling maybe there is some help?
- **Developer tools**
    - part of your Browser
    - suggests: **`//*[@id="mw-content-text"]/ul[1]/li[1]/a`** as XPath expression
- **SelectorGadget** 
    - becomes part of your browser as bookmark or plugin
    - suggests: **`//*+//ul//li[(((count(preceding-sibling::*) + 1) = 1) and parent::*)]//a`** as Xpath expression 


## How XPath works ...
### XPath versus CSS-Selectors
- CSS-Selectors
    - **+ less intimidating**
    - - less explicit
    - - does not support some of XPath's capabilities
- XPath expressions
    - - more intimidating
    - **+ some more capabilities**
    
- What to use? 
    - **use whatever works best for you**
    - **most tasks can be solved with both**
    - Note however, that all of R's HTML/XML parsing/extraction capabilities come down to the use of the XML package in the end. So everything works with XPath under the hood. So some more advanced tasks (e.g. performance matters) have to be solved with XPath anyways. 




# Developer Tools - Live Clicking

# Selector Gadget - Live Clicking



## How XPath works ... 
- **A creative mind**
    - part of every human
    - suggests: 
        - think first
        - looking for similarities
        - looking for differences
        - using every available tool in conjunction
    - ...
        - `<a>`-nodes
        - href should entail `/wiki/`
        - child of `<li>`, child of `<ul>`
        - always 1st child
        - XPath: **`//ul/li/a[1]`**
        - than do further filter by RegEx
        
        
        
# Scenario 2 - Live Coding

## Extracting links of notable political scientists
### get reasonable subset of links

```{r}
ankers <- html_nodes(html, xpath="//ul/li/a[1]")
links  <- html_attr(ankers, "href")
# according to SelectorGagdget should be around 298
length(links) 
```


## Extracting links of notable political scientists
### fine tuning selection of links
```{r}
links_iffer <- 
  seq_along(links) >= 
    seq_along(links)[str_detect(links, "Abramowitz")]  & 
  seq_along(links) <=
    seq_along(links)[str_detect(links, "John_Zaller")] & 
  str_detect(links, "/wiki/")

links_index <- seq_along(links)[links_iffer]
links <- links[links_iffer]

length(links)
```


## Further information on notable political scientists
### names
```{r}
names <- html_attr(ankers, "title")[links_index]
names <- str_replace(names, " \\(.*\\)", "")
# maybe needed: 
names <- iconv(names, "utf8", "latin1")
```

### other information might come from their personal wiki-pages 
- **links to other notable political scientists**
- universities 
- place of birth 
- key puplications,
- ... 

## Downloading PS pages
```{r}
# loop preparation
baseurl <- "http://en.wikipedia.org"
HTML    <- list()
Fname   <- str_c(basename(links), ".html")
URL     <- str_c(baseurl, links)

# loop
for ( i in seq_along(links) ){
  # url
  url <- URL[i]
  # fname
  fname <- Fname[i]
  # download
  if ( !file.exists(fname) ) download.file(url, fname)
  # read in files
  HTML[[i]] <- html(fname)
}
```


## Gathering data on links to other notable PS
```{r}
# loop preparation
connections <- data.frame(from=NULL, to=NULL)

# loop
for ( i in seq_along(HTML))  {
  pslinks          <- html_attr(
                        html_nodes(HTML[[i]], xpath="//a"), 
                      "href")
  links_in_pslinks <- seq_along(links)[links %in% pslinks]
  links_in_pslinks <- links_in_pslinks[links_in_pslinks!=i]
  connections      <- rbind(
                connections, 
                data.frame(
                  from=rep(i, length(links_in_pslinks)), 
                  to=links_in_pslinks
                  ) 
                      )  
}
```

## Gathering data on links to other notable PS
```{r}
# results
names(connections) <- c("from", "to")
head(connections)

# make symmetrical 
connections <- rbind(
                 connections, 
                 data.frame(from=connections$to,
                            to=connections$from)
               )
```


## Plot connections 
```{r}
require(d3Network)
d3SimpleNetwork( connections, 
                 width = 1000, 
                 height = 900, 
                 file="connections.html")
# browseURL("connections.html")
```

## Plot connections 
![](../fig/network1.png)

## Plot connections 
```{r, tidy=TRUE}
d3ForceNetwork(Links = connections, Nodes = data.frame(name=names),
               Source = "from", Target = "to", 
               opacity = 0.9, zoom=T,  width=1000, height=900, 
               file="connections2.html")
# browseURL("connections2.html")
```

## Plot connections 
![](../fig/network2.png)



# Scenario 2 - Extension

## Try yourself ... (1)
### localize notable political scientists
- go through all PS pages and extract university mentions
    - links 
    - ... that have *University*, *School*, *???* in text
- think about how to best store/organize this information
- go get it
- geocode universities similar to scenario 1 



## Try yourself ... (2)
### localize notable political scientists
- Wikipedia pages sometimes entail geographic information
- go through all PS pages and extract all links
    - keep those links that lead to Wikipedia pages
- go through all page links left and look for geolocations connected to the notable political scientist


## Try yourself ... (1 and 2)
### plot all locations gathered on a map 
- have a look at scenario 1 for ideas on how to do it













