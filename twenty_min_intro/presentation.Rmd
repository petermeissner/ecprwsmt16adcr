---
title: "A Fast-Track-Overview on Web Scraping with R"
subtitle: "UseR! 2015"
author: "Peter Mei√üner \\newline Comparative Parliamentary Politics Working Group \\newline University of Konstanz  \\vspace{1em} \\newline https://github.com/petermeissner \\newline http://pmeissner.com \\newline http://www.r-datacollection.com/"
date: "presented: 2015-07-01 / last update: `r Sys.Date()`"
output: 
  beamer_presentation: 
    fonttheme: structurebold
    highlight: zenburn
    keep_tex: yes
fontsize: 9pt
graphics: yes
---

```{r set-options, message=FALSE, error=FALSE, include=FALSE}
options(width = 60)
REDO <- FALSE
library(RSelenium, warn.conflicts = FALSE, quietly=TRUE)
```



# Introduction

$$\includegraphics[width=\textwidth]{webscrapingnetwork.pdf}$$

# Introduction


**phase**     | **problems** | **examples**
--------------|--------------|-----------
**download**  | protocols    | HTTP, HTTPS, POST, GET, ... 
&nbsp;        | procedures   | cookies, authentication, forms, ...
--------------|--------------|------------------------------
**extraction**| parsing      | translating HTML (XML, JSON, ...) into R
&nbsp;        | extraction   | getting the relevant parts
&nbsp;        | cleansing    | cleaning up, restructure, combine





# Conventions

All code examples assume ...

- **`dplyr`**
- **`magrittr`**

... to be loaded via ... 

```{r dplyr and magrittr, warning=FALSE, message=FALSE}
library(dplyr)
library(magrittr)
```

... while all other package dependencies will be made explicit on an example by example base. 


# Reading Text from the Web

```{r get text worker, include=FALSE}
fname_temp <- "cache/base64enc_NEWS.Rdata"
if ( !file.exists(fname_temp) | REDO == TRUE) {
  url <- 
  "http://cran.r-project.org/web/packages/base64enc/NEWS"
  news <- readLines(url) 
  save(news, file=fname_temp)
}else{
  load(fname_temp)
}
```

```{r get text shower, eval = FALSE}
news <- 
  "http://cran.r-project.org/web/packages/base64enc/NEWS"  %>% 
  readLines(url) 
```

```{r, cache = !REDO, message=FALSE}
news %>% extract(1:10) %>%  cat(sep="\n")
```



# Extracting Information from Text
## ... with base R

```{r extract text}
news  %>% 
  substring(7, 16)  %>% 
  grep("\\d{4}.\\d{1,2}.\\d{1,2}", ., value=T)
```


# Extracting Information from Text
## ... with stringr

```{r extract text 2}
library(stringr)
news  %>% 
  str_extract("\\d{4}.\\d{1,2}.\\d{1,2}")
```


# HTML / XML
## ... with rvest
```{r get rpack_html html, include=FALSE}
library(rvest)
fname_temp <- "cache/rpack_html_html.Rdata"
if ( !file.exists(fname_temp) | REDO == TRUE ) {
  url <- "http://cran.r-project.org/web/packages/"
  rpack_html <- paste0(readLines(url), collapse="\n")
  save(rpack_html, file = fname_temp)
}else{
  load(fname_temp)
}
rpack_html <- 
  html(rpack_html)
```

```{r, eval=FALSE}
library(rvest)


rpack_html <- 
  "http://cran.r-project.org/web/packages" %>% 
  html()
```


```{r, cache = !REDO, message=FALSE}
rpack_html %>% class()
```


# HTML / XML
## ... with rvest

```{r, cache = !REDO, message=FALSE}
rpack_html %>% xml_structure(indent = 2)
```


# HTML / XML
## ... with rvest

```{r, cache = !REDO, message=FALSE}
rpack_html %>% html_text()  %>% cat()
```



# Extraction from HTML / XML
## ... with rvest and XPath

```{r, cache = !REDO, message=FALSE}
rpack_html %>%  
  html_node(xpath="//p/a[contains(@href, 'views')]/..")
```


# Extraction from HTML / XML
## ... with rvest and XPath

```{r, cache = !REDO, message=FALSE}
rpack_html %>% 
  html_nodes(xpath="//a") %>% 
  html_attr("href")  %>% 
  extract(1:6)
```


# Extraction from HTML / XML
## ... with rvest convenience functions

```{r, cache = !REDO, message=FALSE}
"http://cran.r-project.org/web/packages/multiplex/index.html"  %>% 
  html() %>% 
  html_table() %>% 
  extract2(1) %>% 
  filter(X1 %in% c("Version:", "Published:", "Author:"))
```


# JSON

```{r, cache = !REDO, message=FALSE}
"https://api.github.com/users/daroczig/repos" %>% 
  readLines(warn=F) %>% 
  substring(1,300) %>% 
  str_wrap(60) %>% 
  cat()
```





# JSON
## ... with jsonlite

```{r, cache = !REDO, message=FALSE}
library(jsonlite)  
fromJSON("https://api.github.com/users/daroczig/repos") %>% 
  select(language) %>% 
  table() %>% 
  sort(decreasing=TRUE)
```




# HTML forms / HTTP methods
## ... with rvest and httr

```{r, cache = !REDO, message=FALSE}
library(rvest)
library(httr)

text    <- 
  "Quirky spud boys can jam after zapping five worthy Polysixes."

mainpage <- html("http://read-able.com") 
```

# HTML forms / HTTP methods
## ... with rvest and httr
```{r, cache = !REDO, message=FALSE}
mainpage %>% 
  html_nodes(xpath="//form") %>% 
  html_attrs()
```

# HTML forms / HTTP methods
## ... with rvest and httr
```{r, cache = !REDO, message=FALSE}
mainpage  %>% 
  html_nodes(
    xpath="//form[@method='post']//*[self::textarea or self::input]"
  ) 
```

# HTML forms / HTTP methods
## ... with rvest and httr

```{r, cache = !REDO, message=FALSE}
response <- 
  POST(
    "http://read-able.com/check.php",
    body=list(directInput = text),
    encode="form"
  )
```

# HTML forms / HTTP methods
## ... with rvest and httr
```{r, cache = !REDO, message=FALSE}
response  %>% 
  extract2("content") %>% 
  rawToChar() %>% 
  html() %>% 
  html_table() %>% 
  extract2(1)
```


# Overcoming the Javascript Barrier
## ... with RSelenium browser automation

```{r, include=FALSE}
new_day <- 
  Sys.Date()  %>% 
  grepl(list.files(pattern="spiegel")) %>% 
  any() %>% 
  !.
```

```{r, message=FALSE, eval=new_day}
library(RSelenium)
checkForServer() # make sure Selenium Server is installed
startServer() 
remDr <- remoteDriver() # defaults firefox
dings <- remDr$open(silent=T) # see: ?remoteDriver !!
remDr$navigate("https://spiegel.de")
remDr$screenshot(
  display   = F, 
  useViewer = F, 
  file      = paste0("spiegel_", Sys.Date(),".png")
)
```


# Overcoming the Javascript Barrier
## ... with RSelenium browser automation

```{r, include=FALSE,  message=FALSE, eval=new_day}
library(png)
png_files   <- list.files(pattern="^spiegel_")
for ( i in seq_along(png_files))  {
  fname <- paste0("small_",png_files[i])
  writePNG(readPNG(png_files[i])[100:900,120:1050,], fname)
}
```



```{r, results='asis', echo=FALSE}
png_files   <- list.files(pattern="^spiegel_")
cat(
  matrix(
    c(
      paste0("\\includegraphics[width=75px]{small_",png_files,"}"),
      rep("", 3 - length(png_files) %% 3)
    ),
    ncol = 3
  )  %>% 
  apply(1, paste, collapse=" ") %>% 
  paste("$$", ., "$$")
)
```


# Authentication
## ... with httr and httpuv
```{r, message=FALSE, warning=FALSE}
library(httpuv)
library(httr)

twitter_token <- oauth1.0_token(
  oauth_endpoints("twitter"), 
  twitter_app <- oauth_app(
    "petermeissneruser2015",
    key = "fP7WB5CcoZNLVQ2Xh8nAdFVAN",
    secret = "PQG1eEJZ65Mb8ANHz8q7yp4MqgAmiAVED90F4ZvQUSTHxiGzPT"
  )
)
```


# Authentication
## ... with httr and httpuv
```{r}
req <- 
  GET(
    paste0(
      "https://api.twitter.com/1.1/search/tweets.json",
      "?q=%23user2015&result_type=recent&count=100"
    ),
    config(token = twitter_token)
  )
```


# Authentication
## ... with httr and httpuv
```{r, eval=FALSE}
tweets <- 
  req  %>% 
  content("parsed")  %>% 
  extract2("statuses") %>% 
  lapply(`[`, "text") %>% 
  unlist(use.names=FALSE) %>% 
  subset(!grepl("^RT ", tweets)) %>% 
  extract(1:15)
```

# Authentication
## ... with httr and httpuv
```{r,include=FALSE}
tweets <- 
  req  %>% 
  content("parsed")  %>% 
  extract2("statuses") %>% 
  lapply(`[`, "text") %>% 
  unlist(use.names=FALSE) %>% 
  subset(!grepl("^RT ", .)) %>% 
  extract(1:15)

tweets <-
  tweets  %>% 
  str_replace_all("[^[:ascii:]]", "") 
```

```{r}
tweets %>% substr(1,60) %>% cat(sep="\n")
```








# Technologies and Packages

- **Regular Expressions / String Handling** 
    - **stringr**,  stringi

- **HTML / XML / XPAth / CSS Selectors**
    - **rvest**, xml2, XML
    
- **JSON**
    - **jsonlite**, RJSONIO, rjson
    
- **HTTP / HTTPS**
    - **httr**, curl, Rcurl
    
- **Javascript / Browser Automation**
    - **RSelenium** 

- **URL** 
    - **urltools** 


# Reads

- **Basics on HTML, XML, JSON, HTTP, RegEx, XPath**
    - Munzert et al. (2014): *Automated Data Collection with R*. Wiley. http://www.r-datacollection.com/

- **curl / libcurl**
    - http://curl.haxx.se/libcurl/c/curl_easy_setopt.html

- **CSS Selectors**
    - W3Schools: http://www.w3schools.com/cssref/css_selectors.asp

- **Packages: httr, rvest, jsonlite, xml2, curl**
    - Readmes, demos and vignettes accompanying the packages 

- **Packages: RCurl and XML**
    - Munzert et al. (2014): *Automated Data Collection with R*. Wiley.
    - Nolan and Temple-Lang (2013): *XML and Web Technologies for Data Science with R*. Springer


# Conclusion

- **Use Mac or Linux** because there will come the time when special characters punch you in the face on R/Windows and according to R-devel this is unlikely to change any time soon.

- Do not listen to guys saying you should use some other language for Web-Scraping. **If you like R, use R** - for any job. 

- Use **stringr, rvest and jsonlite** first and the other packages if needed. 

- If you want to do scraping learn Regular Expressions, file manipulation with R (file.create(), file.remove(), ...), XPath or CSS Selectors and a little HTML-XML-JSON. 

- Web scraping in R has evolved to a convenience state but still is a moving target within a year there might be even more powerful and/or more convenience packages. 

- Before scraping data: (1) Watch for the download button; (2) Have a look at [CRAN Web Technologies Task View](http://cran.r-project.org/web/views/WebTechnologies.html); Look for an API or if maybe someone else has done it before. k


# Thanks
```{r, include=FALSE}
thanks <- function(){
  library(stringr)
  library(dplyr)
  library(magrittr)
  
  packages_used <- c("XML", "RCurl", "httr", "xml2", "rvest", "jsonlite", "RJSONIO", "rjson", "selectr", "urltools", "stringr", "RSelenium", "png", "httpuv", "igraph")
  load("cache/cran_package_tabdata.Rdata")
  
  thanks <- 
    package_data  %>% 
    filter(.$name %in% packages_used) %>% 
    .$Author %>% 
    str_replace_all("\\[.*?\\]|\\(.*?\\)|\r|\n|  |^ ", "") %>% 
    str_split(",|and") %>% 
    unlist %>% 
    str_replace_all("  |^ | $","") %>% 
    str_sort() 
  
  thanks %>% 
    str_c(collapse=", ") %>% 
    str_wrap(60) %>% 
    cat()
  
  invisible(thanks)
}
```

```{r}
thanks()
```

... and the R Community and all the others. 

















